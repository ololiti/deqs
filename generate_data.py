# -*- coding: utf-8 -*-
"""generate_data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16NtQpiatQbGpMN6-wBANkPRh70b-a9Gm
"""

import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader
import numexpr


#length has to be odd#
def generate(length, operators, numbers):
  if length == 1:
      return np.random.choice(numbers)
    
  if length == 3:
      return np.random.choice(numbers) + np.random.choice(operators) + np.random.choice(numbers)

  if (np.random.uniform(0,1) >= 0.6):
      return '(' + generate(length - 2, operators, numbers) + ')'
  else :
      op = np.random.choice(operators)
      position = np.random.choice(range(1, length - 2, 2))
      return generate(position, operators, numbers) + op + generate(length - position - 1, operators, numbers)

# def sign(val):
#     return np.array([1]) if val>0 else np.array([0])

def embed(expression, embedding):
  val = numexpr.evaluate(expression)
  # label = torch.from_numpy(sign(val))
  feat = torch.from_numpy(np.array([[1 if embedding[i]==sym else 0 for i in range(len(embedding))] for sym in expression]))
  return feat.long(), np.array([1.0]) if val>0 else np.array([0])

# def data(sequence, embedding):
#   features = []
#   labels = []
#   for seq in sequence:
#     f, l = embed(seq, embedding)
#     features.append(f)
#     labels.append(l)
#   features = torch.stack(features, 1)
#   labels = torch.cat(labels)
#   return features.T, labels

def list_of_tuples(sequence, embedding):
    list_of_tuples = []
    for seq in sequence:
        f, l = embed(seq, embedding)
        list_of_tuples.append((f, l))
    return list_of_tuples


def get_x_y_list(num_examples, seq_len=31, biased=False):
  if biased:
      operators = np.array(['+', '-', '*', '/', '-', '-'])
  else:
      operators = np.array(['+', '-', '*', '/'])
  numbers = np.array(range(1,10)).astype('str_')
  id = torch.eye(15)
  embedding = [')', '(', '1', '2', '3', '4', '5', '6', '7', '8', '9', '+', '-', '/']
  sequence = []
  for _ in range(num_examples):
    ex = generate(seq_len, operators, numbers)
    try:
      numexpr.evaluate(ex)
    except:
      continue
    sequence.append(ex)
  if biased:
      for i in range(50):
          print(sequence[i])
  return list_of_tuples(sequence, embedding)


def generate_training_data():
    return get_x_y_list(50000)


def generate_test_data():
    return get_x_y_list(5000)


def generate_longer_data():
    return get_x_y_list(5000, 61)


def generate_biased_data():
    return get_x_y_list(5000, biased=True)




def decoder(seq):
    exp = ''
    embedding = [')', '(', '1', '2', '3', '4', '5', '6', '7', '8', '9', '+', '-', '/']
    for sym in seq:
        if torch.sum(sym) == 0:
            exp += '*'
        else:
            index = sym.nonzero(as_tuple=True)[0]
            exp += embedding[index]
    return exp
        


if __name__ == "__main__":
    np.seed(0)
    data = get_x_y_list(500)
    mydataloader = DataLoader(data, batch_size=20)
    for X, y in mydataloader:
        print(X[0])
        print(y[0])
        break




